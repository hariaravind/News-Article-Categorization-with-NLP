{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CATEGORIZATION OF COLLECTION OF NEWS ARTICLES USING NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWF3a70ACEPh"
      },
      "source": [
        "##CATEGORIZATION OF COLLECTION OF NEWS ARTICLES USING NLP\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXZElh9WDLly"
      },
      "source": [
        "In this project, I've used Python to process the textual content of a large collection of news articles with the goal of accurately predicting the correct category for all articles whose categories are currently unknown. \n",
        "\n",
        "Term Frequency â€“ Inverse Document Frequency (TF-IDF) scores for each news article was used for the categorization. \n",
        "\n",
        "###Methodology\n",
        "\n",
        "There are multiple approaches to categorize the text. I've used the average TF-IDF feature vector for each article category. These average TF-IDF feature vectors are used as a basis for calculating the distance between each possible category and each article for which the category is unknown. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq4_DhwGiaAF"
      },
      "source": [
        "###Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x37DDcEQh9lY"
      },
      "source": [
        "#import libraries\n",
        "import numpy as np #used to quickly perform mathematical calculations on vectors\n",
        "import re #regular expressions; used to clean the text data\n",
        "import sqlite3 #used to interact with the database\n",
        "import pandas as pd #allows us to work with data using Pandas dataframes\n",
        "from collections import Counter #used to quickly count letters and words"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKypUIc-ikC5"
      },
      "source": [
        "###Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp2q64y0imRz"
      },
      "source": [
        "#open a connection to the database\n",
        "conn = sqlite3.connect('Dataset.db')\n",
        "\n",
        "#load all documents into a Pandas dataframe named 'df', and use the document_id column as the index\n",
        "sql = 'SELECT * FROM Article'\n",
        "df = pd.read_sql_query(sql, conn, index_col='id')\n",
        "\n",
        "#close database connection\n",
        "conn.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIEAae7Fi7K4"
      },
      "source": [
        "###Preview Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "4U2fEtbWi8wT",
        "outputId": "f1dc7fc9-07d8-4587-bba0-08b07a54abb7"
      },
      "source": [
        "#show the first 10 rows of data\n",
        "df.groupby(['category']).count()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>raw_text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Business</th>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entertainment</th>\n",
              "      <td>197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Politics</th>\n",
              "      <td>239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sports</th>\n",
              "      <td>294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Technology</th>\n",
              "      <td>225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unknown</th>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               raw_text\n",
              "category               \n",
              "Business            270\n",
              "Entertainment       197\n",
              "Politics            239\n",
              "Sports              294\n",
              "Technology          225\n",
              "Unknown            1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8cK5Wj2EZho"
      },
      "source": [
        "The dataset contains 5 categories for the articles. The missing categories have an 'Unknown' tag for which the correct categories are to be predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "rLxKi2fnrwie",
        "outputId": "310dea3e-e79b-462e-8bdc-cf2ce5f6c174"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>raw_text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6347</th>\n",
              "      <td>Politics</td>\n",
              "      <td>Hiding women away in the home hidden behind ve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13840</th>\n",
              "      <td>Sports</td>\n",
              "      <td>Celtic brushed aside Clyde to secure their pla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14775</th>\n",
              "      <td>Unknown</td>\n",
              "      <td>If you have finished Doom 3, Half Life 2 and H...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16641</th>\n",
              "      <td>Unknown</td>\n",
              "      <td>Controversial new UK casinos will be banned fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17511</th>\n",
              "      <td>Unknown</td>\n",
              "      <td>Justine Henin-Hardenne lost to Elena Dementiev...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       category                                           raw_text\n",
              "id                                                                \n",
              "6347   Politics  Hiding women away in the home hidden behind ve...\n",
              "13840    Sports  Celtic brushed aside Clyde to secure their pla...\n",
              "14775   Unknown  If you have finished Doom 3, Half Life 2 and H...\n",
              "16641   Unknown  Controversial new UK casinos will be banned fr...\n",
              "17511   Unknown  Justine Henin-Hardenne lost to Elena Dementiev..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHECfXtFrlpi"
      },
      "source": [
        "###Prepare Data for Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "so6KO0mvrncl",
        "outputId": "f7753b95-7876-47d3-ebfb-936aa2ff21e9"
      },
      "source": [
        "#define a function that will clean the raw input text in preparation for analysis\n",
        "def clean_text(raw_text):\n",
        "  #convert the raw text to lowercase\n",
        "  text = raw_text.lower()\n",
        "  #remove all numbers from the text using a regular expression\n",
        "  text = re.sub(r'[0-9]', ' ', text)\n",
        "  #remove all underscores from the text\n",
        "  text = re.sub(r'\\_', ' ', text)\n",
        "  #remove anything else in the text that isn't a word character or a space (e.g., punctuation, special symbols, etc.)\n",
        "  text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "  #remove any excess whitespace\n",
        "  for _ in range(10):\n",
        "    text = text.replace('  ', ' ')\n",
        "  #remove any leading or trailing space characters\n",
        "  text = text.strip()\n",
        "  #return the clean text\n",
        "  return text\n",
        "\n",
        "#clean the raw text of each article, and store the resulting clean text in a new column \n",
        "df['clean_text'] = [clean_text(raw_text) for raw_text in df.raw_text]\n",
        "\n",
        "#show the cleaned text of the first article\n",
        "df.iloc[0]['clean_text']"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hiding women away in the home hidden behind veils is a backward view of islam president musharraf of pakistan has said during a visit to britain he was speaking to the bbc s newsnight programme a few hours before visiting the pakistani community in manchester my wife is travelling around she is very religious but she is very moderate said general musharraf it comes after pakistan s high commissioner to britain said some pakistanis should integrate more dr maleeha lodhi said people could not expect others to listen to their grievances if they isolated themselves gen musharraf told the bbc some people think that the women should be confined to their houses and put veils on and all that and they should not move out absolutely wrong the pakistani president was also asked whether he thought the war on terror had made the world less safe yes absolutely and i would add that unfortunately we are not addressing the core problems so therefore we can never address it in its totality he said we are fighting it in its immediate context but we are not fighting it in its strategic long term context it is the political disputes and we need to resolve them and also the issue of illiteracy and poverty this combined are breeding grounds of extremism and terrorism on monday the pakistani president met prime minister tony blair at downing street on his first official visit to london he is due to visit the pakistani community in manchester on tuesday afternoon the mirror newspaper said on tuesday it had been handed a sensitive dossier outling the details of gen musharraf s visit to britain the paper said the document had been found in a london street by a member of the public it said the dossier contained details about his movements and also confidential police radio channels call signs and codes speaking in london on monday gen musharraf said al qaeda was on the run in pakistan but standing next to mr blair he added that it was crucial to tackle the core of what creates terrorists what creates an extremist militant environment which then leads on to terrorism that is the resolution of political disputes mr blair said the two leaders had talked about afghanistan the wider war on terror the situation in the middle east and the ongoing dispute over kashmir we agreed that in afghanistan there is some cause for optimism about the progress that has been made there said mr blair in respect of iraq we agreed that whatever the issues of the past the important thing now is to see the strategy through and ensure that iraq is capable of becoming a stable and democratic state'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC0ikTikx_We"
      },
      "source": [
        "###Compute Raw Letter Frequencies for Each Language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5FNsuvWyAdb"
      },
      "source": [
        "#define a function that will compute the raw letter frequencies for the input texts as well as the total number of letters appearing in the input texts\n",
        "def letter_counts(input_texts):\n",
        "  all_text = ' '.join(input_texts) #join all of the input texts into one big string\n",
        "  letter_counts = Counter(all_text.replace(' ', '')) #count all letters in the text (excluding spaces)\n",
        "  #return letter counts (sorted from most common to least common), and the total number of letters\n",
        "  return letter_counts.most_common(), sum(letter_counts.values()) \n",
        "\n",
        "\n",
        "#get letter counts for each language\n",
        "df_letter_counts, total_letters = letter_counts(df.clean_text)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGzusxIXygEZ",
        "outputId": "b65a4490-5691-411b-b8fe-751708ae9e4d"
      },
      "source": [
        "df_letter_counts"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('e', 476074),\n",
              " ('t', 347366),\n",
              " ('a', 327937),\n",
              " ('o', 291803),\n",
              " ('i', 289103),\n",
              " ('n', 277156),\n",
              " ('s', 268268),\n",
              " ('r', 248520),\n",
              " ('h', 186120),\n",
              " ('l', 167314),\n",
              " ('d', 154621),\n",
              " ('c', 125245),\n",
              " ('u', 106896),\n",
              " ('m', 105290),\n",
              " ('p', 85065),\n",
              " ('g', 83122),\n",
              " ('f', 82762),\n",
              " ('w', 75363),\n",
              " ('b', 70700),\n",
              " ('y', 65325),\n",
              " ('v', 42983),\n",
              " ('k', 31394),\n",
              " ('x', 8246),\n",
              " ('j', 7868),\n",
              " ('q', 3261),\n",
              " ('z', 2761)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PQJpoo28n3m"
      },
      "source": [
        "###Build a vocabulary for all of the news articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8AJMQ7h8pHL"
      },
      "source": [
        "#build a vocabulary of words\n",
        "all_text = ' '.join(df.clean_text) #join all of the texts into one big string\n",
        "words = all_text.split() #split the text into words\n",
        "word_frequencies = Counter(words) #count all words in the text\n",
        "vocabulary = list(word_frequencies.keys()) #get a list of all unique words"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZvYuI1_82oa",
        "outputId": "14355bba-554a-4909-d4a6-e40fba5e550b"
      },
      "source": [
        "len(vocabulary)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27762"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPhU6qmsI1V0"
      },
      "source": [
        "#define a class that we can use to hold information about each document\n",
        "class Document:\n",
        "  def __init__(self, id, category, word_frequencies, total_words):\n",
        "    self.id = id #the document's unique ID number\n",
        "    self.category = category #the document's category\n",
        "    self.predicted_category = None\n",
        "    self.total_words = total_words #the total number of words in the document\n",
        "    self.word_frequencies = word_frequencies #holds raw frequencies for each word in the vocabulary\n",
        "    self.term_frequencies = None\n",
        "    self.tfidf_scores = None"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi6mzslA9TUO"
      },
      "source": [
        "#sort the vocabulary to ensure that we all get consistent results!\n",
        "vocabulary.sort()\n",
        "#define a collection (list) to hold our Document objects\n",
        "documents = []\n",
        "#create a Document object for each document in the corpus\n",
        "for row in df.itertuples(): #for each row in the dataframe\n",
        "  words = row.clean_text.split() #split the (clean) text into words\n",
        "  document_word_frequencies = Counter(words) #count all words in the document's (clean) text\n",
        "  total_words = sum(document_word_frequencies.values()) #compute the total number of words in the document\n",
        "  \n",
        "  vocabulary_word_frequencies = []\n",
        "  for vocabulary_word in vocabulary:\n",
        "    #if this vocabulary word exists in the document\n",
        "    if vocabulary_word in document_word_frequencies:\n",
        "      #add the raw document frequency for this vocabulary word to the collection\n",
        "      vocabulary_word_frequencies.append(document_word_frequencies[vocabulary_word])\n",
        "    else: #if this vocabulary word doesn't exist in the document\n",
        "      #add a value of zero for this vocabulary word to the collection\n",
        "      vocabulary_word_frequencies.append(0)      \n",
        "  #add a new Document object for this document to the collection\n",
        "  documents.append(Document(row.Index, row.category, vocabulary_word_frequencies, total_words))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yPKH8-Nz0EVu",
        "outputId": "c8076cfe-6a8c-463b-9a81-940b505c8cac"
      },
      "source": [
        "documents[9].category"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Technology'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXKNFHKGK2Bz"
      },
      "source": [
        "#for each document in the 'documents' collection\n",
        "for document in documents:\n",
        "  #compute the unigram probability distributions for this document\n",
        "  document.term_frequencies = np.array(document.word_frequencies) / document.total_words"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I2H9OxbV-fC"
      },
      "source": [
        "#Calculate an IDF Score for each word in the vocabulary\n",
        "idf_scores = []\n",
        "for i in range(len(vocabulary)):\n",
        "  number_of_documents = 0\n",
        "  for document in documents:\n",
        "    if document.word_frequencies[i] > 0:\n",
        "      number_of_documents +=1\n",
        "  idf = np.log(len(documents) / number_of_documents)\n",
        "  idf_scores.append(idf)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2FN_ezZ4kCt",
        "outputId": "17c92617-57f5-4f02-fcc9-64e409942501"
      },
      "source": [
        "len(idf_scores)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27762"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzW265Z55DE3",
        "outputId": "93b1f290-1caa-4237-b362-dad78409d2f0"
      },
      "source": [
        "vocabulary[:15]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'aa',\n",
              " 'aaa',\n",
              " 'aaas',\n",
              " 'aac',\n",
              " 'aadc',\n",
              " 'aaliyah',\n",
              " 'aaltra',\n",
              " 'aamir',\n",
              " 'aan',\n",
              " 'aara',\n",
              " 'aarhus',\n",
              " 'aaron',\n",
              " 'abacus',\n",
              " 'abandon']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMzyrlI047KI",
        "outputId": "3b182c5b-50d5-4bb8-818c-3e56e69eca42"
      },
      "source": [
        "idf_scores[:15]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.007216991180223392,\n",
              " 7.707512194600341,\n",
              " 5.6280706529205045,\n",
              " 5.761602045545027,\n",
              " 7.014365014040395,\n",
              " 7.707512194600341,\n",
              " 7.707512194600341,\n",
              " 7.707512194600341,\n",
              " 7.707512194600341,\n",
              " 7.707512194600341,\n",
              " 7.707512194600341,\n",
              " 7.707512194600341,\n",
              " 6.09807428216624,\n",
              " 7.014365014040395,\n",
              " 6.608899905932231]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYG5hBLzEHbG"
      },
      "source": [
        "###Calculate TF-IDF Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ss26ffK6y9J"
      },
      "source": [
        "#Calculate TF-IDF Scores\n",
        "idf_scores = np.array(idf_scores)\n",
        "for document in documents:\n",
        "  document.tfidf_scores = np.array(document.term_frequencies) * idf_scores"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPrVi2d8JILN"
      },
      "source": [
        "#define a dictionary that holds each category's name (keys) and average word probability distribution (values).\n",
        "#The probability distributions are all numpy arrays of the same size as the vocabulary. All elements of each probability distribuation are initialized to zero.\n",
        "category_tfidf_scores = {'Business': np.zeros(len(vocabulary)), 'Entertainment': np.zeros(len(vocabulary)), 'Politics': np.zeros(len(vocabulary)), 'Sports': np.zeros(len(vocabulary)), 'Technology': np.zeros(len(vocabulary))}\n",
        "#define a dictionary to hold the number of documents for each category\n",
        "document_counts = {'Business': 0, 'Entertainment': 0, 'Politics': 0, 'Sports': 0, 'Technology': 0}\n",
        "#for each document in the corpus\n",
        "for document in documents:\n",
        "  #if the category of this document is known\n",
        "  if document.category != 'Unknown':\n",
        "    #increment the document count for this category\n",
        "    document_counts[document.category] += 1\n",
        "    #add this document's word probabilities to the running sum for the corresponding distribution \n",
        "    #for the document's category\n",
        "    category_tfidf_scores[document.category] += document.tfidf_scores\n",
        "#compute the average word probability distributions for each category by dividing the summed probabilities\n",
        "#by the number of documents for each category\n",
        "for category in category_tfidf_scores:\n",
        "  category_tfidf_scores[category] /= document_counts[category]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uHo8V_y8rpm",
        "outputId": "3f48536d-382c-4fd2-aea2-f7fd4791d512"
      },
      "source": [
        "category_tfidf_scores['Entertainment']"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00015555, 0.        , 0.        , ..., 0.00018992, 0.        ,\n",
              "       0.00017036])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFPG2tn9LWge"
      },
      "source": [
        "#define a function to compute the Euclidean distance between two points \n",
        "#(where each point is defined as a vector)\n",
        "def get_distance(point1, point2):\n",
        "  return np.sqrt(np.sum(np.square(point1 - point2)))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs2SkYikLl81",
        "outputId": "a0f81d6d-82f5-4727-d85e-c100868e16c7"
      },
      "source": [
        "#for each document in the corpus\n",
        "number_of_accurate_predictions = 0\n",
        "number_of_known_articles = 0\n",
        "for document in documents:\n",
        "  min_distance = np.inf\n",
        "  best_category = None\n",
        "  for category in category_tfidf_scores:\n",
        "    distance = get_distance(document.tfidf_scores, category_tfidf_scores[category])\n",
        "    if distance < min_distance:\n",
        "      min_distance = distance\n",
        "      best_category = category\n",
        "  document.predicted_category = best_category\n",
        "  if document.category != 'Unknown':\n",
        "    number_of_known_articles += 1\n",
        "    if document.category == document.predicted_category:\n",
        "      number_of_accurate_predictions +=1\n",
        "\n",
        "print('Overall accuracy', number_of_accurate_predictions / number_of_known_articles)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall accuracy 0.9844897959183674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReWSlwaFERVy"
      },
      "source": [
        "###Conclusion and Further Development\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6wZfDjHEp1L"
      },
      "source": [
        "The overall accuracy for the known articles using TF-IDF scores are obtained to be 98.45%. \n",
        "\n",
        "Google introduced an open-sourced a neural network-based technique for natural language processing (NLP) pre-training called [Bidirectional Encoder Representations from Transformers - BERT](https://blog.google/products/search/search-language-understanding-bert/). The above can be re-evaluated using BERT and the scores can be compared."
      ]
    }
  ]
}